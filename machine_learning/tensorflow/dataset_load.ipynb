{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yujin/anaconda3/envs/machine_learning/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-08-04 15:02:42.777121: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1226\n",
      "['abstract_reasoning', 'accentdb', 'aeslc', 'aflw2k3d', 'ag_news_subset']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "print(len(tfds.list_builders()))\n",
    "print(tfds.list_builders()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "현재 1226개의 데이터셋이 사용 가능하다,\n",
    "처음 5개 데이터셋의 리스트는 ['abstract_reasoning', 'accentdb', 'aeslc', 'aflw2k3d', 'ag_news_subset'] 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeaturesDict({\n",
      "    'attributes': FeaturesDict({\n",
      "        '5_o_Clock_Shadow': bool,\n",
      "        'Arched_Eyebrows': bool,\n",
      "        'Attractive': bool,\n",
      "        'Bags_Under_Eyes': bool,\n",
      "        'Bald': bool,\n",
      "        'Bangs': bool,\n",
      "        'Big_Lips': bool,\n",
      "        'Big_Nose': bool,\n",
      "        'Black_Hair': bool,\n",
      "        'Blond_Hair': bool,\n",
      "        'Blurry': bool,\n",
      "        'Brown_Hair': bool,\n",
      "        'Bushy_Eyebrows': bool,\n",
      "        'Chubby': bool,\n",
      "        'Double_Chin': bool,\n",
      "        'Eyeglasses': bool,\n",
      "        'Goatee': bool,\n",
      "        'Gray_Hair': bool,\n",
      "        'Heavy_Makeup': bool,\n",
      "        'High_Cheekbones': bool,\n",
      "        'Male': bool,\n",
      "        'Mouth_Slightly_Open': bool,\n",
      "        'Mustache': bool,\n",
      "        'Narrow_Eyes': bool,\n",
      "        'No_Beard': bool,\n",
      "        'Oval_Face': bool,\n",
      "        'Pale_Skin': bool,\n",
      "        'Pointy_Nose': bool,\n",
      "        'Receding_Hairline': bool,\n",
      "        'Rosy_Cheeks': bool,\n",
      "        'Sideburns': bool,\n",
      "        'Smiling': bool,\n",
      "        'Straight_Hair': bool,\n",
      "        'Wavy_Hair': bool,\n",
      "        'Wearing_Earrings': bool,\n",
      "        'Wearing_Hat': bool,\n",
      "        'Wearing_Lipstick': bool,\n",
      "        'Wearing_Necklace': bool,\n",
      "        'Wearing_Necktie': bool,\n",
      "        'Young': bool,\n",
      "    }),\n",
      "    'identity': FeaturesDict({\n",
      "        'Identity_No': int64,\n",
      "    }),\n",
      "    'image': Image(shape=(218, 178, 3), dtype=uint8),\n",
      "    'landmarks': FeaturesDict({\n",
      "        'lefteye_x': int64,\n",
      "        'lefteye_y': int64,\n",
      "        'leftmouth_x': int64,\n",
      "        'leftmouth_y': int64,\n",
      "        'nose_x': int64,\n",
      "        'nose_y': int64,\n",
      "        'righteye_x': int64,\n",
      "        'righteye_y': int64,\n",
      "        'rightmouth_x': int64,\n",
      "        'rightmouth_y': int64,\n",
      "    }),\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "celeba_bldr = tfds.builder('celeb_a')\n",
    "print(celeba_bldr.info.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image(shape=(218, 178, 3), dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "print(celeba_bldr.info.features['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@inproceedings{conf/iccv/LiuLWT15,\n",
      "  added-at = {2018-10-09T00:00:00.000+0200},\n",
      "  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},\n",
      "  biburl = {https://www.bibsonomy.org/bibtex/250e4959be61db325d2f02c1d8cd7bfbb/dblp},\n",
      "  booktitle = {ICCV},\n",
      "  crossref = {conf/iccv/2015},\n",
      "  ee = {http://doi.ieeecomputersociety.org/10.1109/ICCV.2015.425},\n",
      "  interhash = {3f735aaa11957e73914bbe2ca9d5e702},\n",
      "  intrahash = {50e4959be61db325d2f02c1d8cd7bfbb},\n",
      "  isbn = {978-1-4673-8391-2},\n",
      "  keywords = {dblp},\n",
      "  pages = {3730-3738},\n",
      "  publisher = {IEEE Computer Society},\n",
      "  timestamp = {2018-10-11T11:43:28.000+0200},\n",
      "  title = {Deep Learning Face Attributes in the Wild.},\n",
      "  url = {http://dblp.uni-trier.de/db/conf/iccv/iccv2015.html#LiuLWT15},\n",
      "  year = 2015\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(celeba_bldr.info.citation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 설명\n",
    "image, landmarks, attributes 세 개의 키를 가진 딕셔너리\n",
    "shape=(218, 178, 3) 3차원 데이터셋\n",
    "'image'키는 유명인사의 얼굴 이미지를 담고있다.\n",
    "'landmarks'키는 얼굴에서 추출한 위치로 구성된 딕셔너리 (ex. 눈,코,입 위치)\n",
    "'attributes' 키는 이미지에 있는 사람의 얼굴속성 (ex. 표정, 화장, 머리카락)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 1.39 GiB (download: 1.39 GiB, generated: 1.63 GiB, total: 3.01 GiB) to /Users/yujin/tensorflow_datasets/celeb_a/2.1.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Size...:  37%|███▋      | 15/41 [00:14<00:25,  1.03 MiB/s]rl]\n",
      "Dl Completed...:  60%|██████    | 3/5 [00:14<00:09,  4.84s/ url]\n"
     ]
    },
    {
     "ename": "NonMatchingChecksumError",
     "evalue": "Artifact https://drive.google.com/uc?export=download&id=0B7EVK8r0v71pZjFTYXZWM3FlRnM, downloaded to /Users/yujin/tensorflow_datasets/downloads/ucexport_download_id_0B7EVK8r0v71pZjFTYXZWM3FlDDaXUAQO8EGH_a7VqGNLRtW52mva1LzDrb-V723OQN8.tmp.2e5a7531dc6d4824af89dd6c0727d0a7/download, has wrong checksum:\n* Expected: UrlInfo(size=1.34 GiB, checksum='46fb89443c578308acf364d7d379fe1b9efb793042c0af734b6112e4fd3a8c74', filename='img_align_celeba.zip')\n* Got: UrlInfo(size=2.37 KiB, checksum='66ec5670a65e5754c2e6e0ed6c96c97f008ee4ccb3557e4683b7b3b5e65fefef', filename='download')\nTo debug, see: https://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNonMatchingChecksumError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mceleba_bldr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.9/site-packages/tensorflow_datasets/core/logging/__init__.py:166\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_call()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m   metadata\u001b[38;5;241m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builder.py:691\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[1;32m    689\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mread_from_directory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir)\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 691\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m   \u001b[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[1;32m    697\u001b[0m   \u001b[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[1;32m    698\u001b[0m   \u001b[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[1;32m    699\u001b[0m   \u001b[38;5;66;03m# when reading from package data.\u001b[39;00m\n\u001b[1;32m    700\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdownload_size \u001b[38;5;241m=\u001b[39m dl_manager\u001b[38;5;241m.\u001b[39mdownloaded_size\n",
      "File \u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builder.py:1547\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1546\u001b[0m   optional_pipeline_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1547\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=unexpected-keyword-arg\u001b[39;49;00m\n\u001b[1;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptional_pipeline_kwargs\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[38;5;66;03m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[39;00m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;66;03m# https://github.com/tensorflow/datasets/issues/2537\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;66;03m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001b[39;00m\n\u001b[1;32m   1553\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m split_builder\u001b[38;5;241m.\u001b[39mnormalize_legacy_split_generators(\n\u001b[1;32m   1554\u001b[0m     split_generators\u001b[38;5;241m=\u001b[39msplit_generators,\n\u001b[1;32m   1555\u001b[0m     generator_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_examples,\n\u001b[1;32m   1556\u001b[0m     is_beam\u001b[38;5;241m=\u001b[39m\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, BeamBasedBuilder),\n\u001b[1;32m   1557\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.9/site-packages/tensorflow_datasets/datasets/celeb_a/celeb_a_dataset_builder.py:100\u001b[0m, in \u001b[0;36mBuilder._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_split_generators\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager):\n\u001b[0;32m--> 100\u001b[0m   downloaded_dirs \u001b[38;5;241m=\u001b[39m \u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimg_align_celeba\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_ALIGNED_DATA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlist_eval_partition\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mEVAL_LIST\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlist_attr_celeba\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mATTR_DATA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlandmarks_celeba\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mLANDMARKS_DATA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43midentity_celeba\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mIDENTITY_DATA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m   \u001b[38;5;66;03m# Load all images in memory (~1 GiB)\u001b[39;00m\n\u001b[1;32m    109\u001b[0m   \u001b[38;5;66;03m# Use split to convert: `img_align_celeba/000005.jpg` -> `000005.jpg`\u001b[39;00m\n\u001b[1;32m    110\u001b[0m   all_images \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    111\u001b[0m       os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(k)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]: img\n\u001b[1;32m    112\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m k, img \u001b[38;5;129;01min\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39miter_archive(\n\u001b[1;32m    113\u001b[0m           downloaded_dirs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_align_celeba\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    114\u001b[0m       )\n\u001b[1;32m    115\u001b[0m   }\n",
      "File \u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.9/site-packages/tensorflow_datasets/core/download/download_manager.py:601\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# Add progress bar to follow the download state\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_downloader\u001b[38;5;241m.\u001b[39mtqdm():\n\u001b[0;32m--> 601\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_promise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.9/site-packages/tensorflow_datasets/core/download/download_manager.py:831\u001b[0m, in \u001b[0;36m_map_promise\u001b[0;34m(map_fn, all_inputs)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[1;32m    828\u001b[0m all_promises \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    829\u001b[0m     map_fn, all_inputs\n\u001b[1;32m    830\u001b[0m )  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[0;32m--> 831\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mtree_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_promises\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.9/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     [func(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.9/site-packages/tree/__init__.py:435\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    433\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m--> 435\u001b[0m                     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "File \u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.9/site-packages/tensorflow_datasets/core/download/download_manager.py:832\u001b[0m, in \u001b[0;36m_map_promise.<locals>.<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001b[39;00m\n\u001b[1;32m    828\u001b[0m all_promises \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    829\u001b[0m     map_fn, all_inputs\n\u001b[1;32m    830\u001b[0m )  \u001b[38;5;66;03m# Apply the function\u001b[39;00m\n\u001b[1;32m    831\u001b[0m res \u001b[38;5;241m=\u001b[39m tree_utils\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m--> 832\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, all_promises\n\u001b[1;32m    833\u001b[0m )  \u001b[38;5;66;03m# Wait promises\u001b[39;00m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.9/site-packages/promise/promise.py:512\u001b[0m, in \u001b[0;36mPromise.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    510\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target()\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(timeout \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_TIMEOUT)\n\u001b[0;32m--> 512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target_settled_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_raise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.9/site-packages/promise/promise.py:516\u001b[0m, in \u001b[0;36mPromise._target_settled_value\u001b[0;34m(self, _raise)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_target_settled_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, _raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;66;03m# type: (bool) -> Any\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_settled_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_raise\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.9/site-packages/promise/promise.py:226\u001b[0m, in \u001b[0;36mPromise._settled_value\u001b[0;34m(self, _raise)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _raise:\n\u001b[1;32m    225\u001b[0m     raise_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fulfillment_handler0\n\u001b[0;32m--> 226\u001b[0m     \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraise_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_traceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fulfillment_handler0\n",
      "File \u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.9/site-packages/six.py:719\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    721\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.9/site-packages/promise/promise.py:87\u001b[0m, in \u001b[0;36mtry_catch\u001b[0;34m(handler, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtry_catch\u001b[39m(handler, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# type: (Callable, Any, Any) -> Union[Tuple[Any, None], Tuple[None, Tuple[Exception, Optional[TracebackType]]]]\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     89\u001b[0m         tb \u001b[38;5;241m=\u001b[39m exc_info()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.9/site-packages/tensorflow_datasets/core/download/download_manager.py:408\u001b[0m, in \u001b[0;36mDownloadManager._download.<locals>.<lambda>\u001b[0;34m(dl_result)\u001b[0m\n\u001b[1;32m    402\u001b[0m   future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_downloader\u001b[38;5;241m.\u001b[39mdownload(\n\u001b[1;32m    403\u001b[0m       url, download_tmp_dir, verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_ssl\n\u001b[1;32m    404\u001b[0m   )\n\u001b[1;32m    406\u001b[0m \u001b[38;5;66;03m# Post-process the result\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mthen(\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m dl_result: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_register_or_validate_checksums\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=g-long-lambda\u001b[39;49;00m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomputed_url_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_url_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_url_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchecksum_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.9/site-packages/tensorflow_datasets/core/download/download_manager.py:465\u001b[0m, in \u001b[0;36mDownloadManager._register_or_validate_checksums\u001b[0;34m(self, path, url, expected_url_info, computed_url_info, checksum_path, url_path)\u001b[0m\n\u001b[1;32m    454\u001b[0m   checksum_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dl_path(url, computed_url_info\u001b[38;5;241m.\u001b[39mchecksum)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m   \u001b[38;5;66;03m# Eventually validate checksums\u001b[39;00m\n\u001b[1;32m    457\u001b[0m   \u001b[38;5;66;03m# Note:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    463\u001b[0m   \u001b[38;5;66;03m#   download). This is expected as it might mean the downloaded file\u001b[39;00m\n\u001b[1;32m    464\u001b[0m   \u001b[38;5;66;03m#   was corrupted. Note: The tmp file isn't deleted to allow inspection.\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m   \u001b[43m_validate_checksums\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m      \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m      \u001b[49m\u001b[43mexpected_url_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_url_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomputed_url_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomputed_url_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m      \u001b[49m\u001b[43mforce_checksums_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_checksums_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rename_and_get_final_dl_path(\n\u001b[1;32m    474\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    475\u001b[0m     path\u001b[38;5;241m=\u001b[39mpath,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    479\u001b[0m     url_path\u001b[38;5;241m=\u001b[39murl_path,\n\u001b[1;32m    480\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.9/site-packages/tensorflow_datasets/core/download/download_manager.py:809\u001b[0m, in \u001b[0;36m_validate_checksums\u001b[0;34m(url, path, computed_url_info, expected_url_info, force_checksums_validation)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    798\u001b[0m     expected_url_info\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m computed_url_info\n\u001b[1;32m    800\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m expected_url_info \u001b[38;5;241m!=\u001b[39m computed_url_info\n\u001b[1;32m    801\u001b[0m ):\n\u001b[1;32m    802\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    803\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArtifact \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, downloaded to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, has wrong checksum:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    804\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m* Expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_url_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    808\u001b[0m   )\n\u001b[0;32m--> 809\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m NonMatchingChecksumError(msg)\n",
      "\u001b[0;31mNonMatchingChecksumError\u001b[0m: Artifact https://drive.google.com/uc?export=download&id=0B7EVK8r0v71pZjFTYXZWM3FlRnM, downloaded to /Users/yujin/tensorflow_datasets/downloads/ucexport_download_id_0B7EVK8r0v71pZjFTYXZWM3FlDDaXUAQO8EGH_a7VqGNLRtW52mva1LzDrb-V723OQN8.tmp.2e5a7531dc6d4824af89dd6c0727d0a7/download, has wrong checksum:\n* Expected: UrlInfo(size=1.34 GiB, checksum='46fb89443c578308acf364d7d379fe1b9efb793042c0af734b6112e4fd3a8c74', filename='img_align_celeba.zip')\n* Got: UrlInfo(size=2.37 KiB, checksum='66ec5670a65e5754c2e6e0ed6c96c97f008ee4ccb3557e4683b7b3b5e65fefef', filename='download')\nTo debug, see: https://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror"
     ]
    }
   ],
   "source": [
    "celeba_bldr.download_and_prepare()\n",
    "# 데이터셋을 위해 지정된 파일에 데이터셋을 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "파일이 존재하지 않습니다. 수동으로 다운로드 후 지정된 경로에 저장하세요.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m img_align_celeba_zip \u001b[38;5;241m=\u001b[39m download_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_align_celeba.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 다운로드한 파일이 있는지 확인\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m img_align_celeba_zip\u001b[38;5;241m.\u001b[39mexists(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m파일이 존재하지 않습니다. 수동으로 다운로드 후 지정된 경로에 저장하세요.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 데이터셋 빌더 설정\u001b[39;00m\n\u001b[1;32m     12\u001b[0m celeba_bldr \u001b[38;5;241m=\u001b[39m tfds\u001b[38;5;241m.\u001b[39mbuilder(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mceleb_a\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: 파일이 존재하지 않습니다. 수동으로 다운로드 후 지정된 경로에 저장하세요."
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import pathlib\n",
    "\n",
    "# 수동으로 다운로드한 파일 경로 설정\n",
    "download_dir = pathlib.Path('~/tensorflow_datasets/manual_dir')\n",
    "img_align_celeba_zip = download_dir / 'img_align_celeba.zip'\n",
    "\n",
    "# 다운로드한 파일이 있는지 확인\n",
    "assert img_align_celeba_zip.exists(), \"파일이 존재하지 않습니다. 수동으로 다운로드 후 지정된 경로에 저장하세요.\"\n",
    "\n",
    "# 데이터셋 빌더 설정\n",
    "celeba_bldr = tfds.builder('celeb_a')\n",
    "celeba_bldr.download_and_prepare(download_dir=download_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
